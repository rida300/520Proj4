#!/bin/bash

## Specify the amount of RAM needed _per_core_. Default is 1G
#SBATCH --mem-per-cpu=4G

## Specify the maximum runtime in DD-HH:MM:SS form. Default is 1 hour (1:00:00)
#SBATCH --time=6:00:00


## number of cores/nodes: NOT FINISHED
## quick note here. Jobs requesting 16 or fewer cores tend to get scheduled
## fairly quickly. If you need a job that requires more than that, you might
## benefit from emailing us at beocat@cs.ksu.edu to see how we can assist in
## getting your job scheduled in a reasonable amount of time. Default is
##SBATCH --cpus-per-task=1
##SBATCH --cpus-per-task=12
##SBATCH --nodes=2 --tasks-per-node=1
##SBATCH --tasks=20

## Constraints for this job. Maybe you need to run on the elves
#SBATCH --constraint=elves

## Output file name. Default is slurm-%j.out where %j is the job id.
#SBATCH --output=MyJobTitle.o%j

## Split the errors into a seperate file. Default is the same as output
#SBATCH --error=MyJobTitle.e%j

## Name my job, to make it easier to find in the queue
#SBATCH -J OMP

## Send email when certain criteria are met.
## Valid type values are NONE, BEGIN, END, FAIL, REQUEUE, ALL (equivalent to
## BEGIN, END, FAIL, REQUEUE,  and  STAGE_OUT),  STAGE_OUT  (burst buffer stage
## out and teardown completed), TIME_LIMIT, TIME_LIMIT_90 (reached 90 percent
## of time limit), TIME_LIMIT_80 (reached 80 percent of time limit),
## TIME_LIMIT_50 (reached 50 percent of time limit) and ARRAY_TASKS (send
## emails for each array task). Multiple type values may be specified in a
## comma separated list. Unless the  ARRAY_TASKS  option  is specified, mail
## notifications on job BEGIN, END and FAIL apply to a job array as a whole
## rather than generating individual email messages for each task in the job
## array.
#SBATCH --mail-type=ALL

## Email address to send the email to based on the above line.
## Default is to send the mail to the e-mail address entered on the account
## request form.
#SBATCH --mail-user myemail@ksu.edu

## And finally, we run the job we came here to do.



make clean
make omp_exec

echo 'LSS with omp threads ... '

$HOME/520Proj4/proj4/omp_exec 

## OR, for the case of MPI-capable jobs
## mpirun $HOME/path/MpiJobName


